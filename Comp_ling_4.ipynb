{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00fad453",
   "metadata": {},
   "source": [
    "# Домашнее задание № 4. Языковые модели"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d056af4",
   "metadata": {},
   "source": [
    "## Задание 1 (8 баллов)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f532a8",
   "metadata": {},
   "source": [
    "В семинаре для генерации мы использовали предположение маркова и считали, что слово зависит только от 1 предыдущего слова. Но ничто нам не мешает попробовать увеличить размер окна и учитывать два или даже три прошлых слова. Для них мы еще сможем собрать достаточно статистик и, логично предположить, что качество сгенерированного текста должно вырасти."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de743d1d",
   "metadata": {},
   "source": [
    "Попробуйте сделать языковую модель, которая будет учитывать два предыдущих слова при генерации текста.\n",
    "Сгенерируйте несколько текстов (3-5) и расчитайте перплексию получившейся модели. \n",
    "Можно использовать данные из семинара или любые другие (сопоставимые или большие по объему). Перплексию рассчитывайте на 10-50 отложенных предложениях (они не должны использоваться при сборе статистик).\n",
    "\n",
    "\n",
    "Подсказки:  \n",
    "    - нужно будет добавить еще один тэг <start>  \n",
    "    - еще одна матрица не нужна, можно по строкам хранить биграмы, а по колонкам униграммы  \n",
    "    - тексты должны быть очень похожи на нормальные (если у вас получается рандомная каша, вы что-то делаете не так). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d078056d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "from razdel import sentenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from razdel import tokenize as razdel_tokenize\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix, csc_matrix, lil_matrix\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6afcef88",
   "metadata": {},
   "outputs": [],
   "source": [
    "dvach = open('2ch_corpus.txt', encoding = 'utf-8').read()\n",
    "news = open('lenta.txt', encoding = 'utf-8').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "944ee988",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(text):\n",
    "    normalized_text = [word.text.strip(punctuation) for word in razdel_tokenize(text)]\n",
    "    normalized_text = [word.lower() for word in normalized_text if word and len(word) < 20 ]\n",
    "    return normalized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f550df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_dvach = normalize(dvach)\n",
    "norm_news = normalize(news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "26af18c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_dvach = Counter(norm_dvach)\n",
    "vocab_news = Counter(norm_news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ded6f7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngrammer(tokens, n=3):\n",
    "    ngrams = []\n",
    "    for i in range(0, len(tokens)-n+1):\n",
    "        ngrams.append(' '.join(tokens[i:i+n]))\n",
    "    return ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "86f2413b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_dvach = sent_tokenize(dvach)\n",
    "sentences_news = sent_tokenize(news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "fe3b5334",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dvach = sentences_dvach[-10:]\n",
    "test_news = sentences_news[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "988bdce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_dvach = [['<start>'] + ['<start>'] + normalize(text) + ['<end>'] for text in sentences_dvach[:-10]]\n",
    "sentences_news = [['<start>'] + ['<start>'] + normalize(text) + ['<end>'] for text in sentences_news[:-10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "d865a3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "unigrams_dvach = Counter()\n",
    "bigrams_dvach = Counter()\n",
    "trigrams_dvach = Counter()\n",
    "\n",
    "for sentence in sentences_dvach:\n",
    "    unigrams_dvach.update(sentence)\n",
    "    bigrams_dvach.update(ngrammer(sentence, n=2))\n",
    "    trigrams_dvach.update(ngrammer(sentence))\n",
    "\n",
    "unigrams_news = Counter()\n",
    "bigrams_news = Counter()\n",
    "trigrams_news = Counter()\n",
    "\n",
    "for sentence in sentences_news:\n",
    "    unigrams_news.update(sentence)\n",
    "    bigrams_news.update(ngrammer(sentence, n=2))\n",
    "    trigrams_news.update(ngrammer(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "3c55216c",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_dvach = lil_matrix((len(bigrams_dvach), len(unigrams_dvach)))\n",
    "\n",
    "id2word_dvach = list(unigrams_dvach)\n",
    "word2id_dvach = {word:i for i, word in enumerate(id2word_dvach)}\n",
    "\n",
    "id2bigram_dvach = list(bigrams_dvach)\n",
    "bigram2id_dvach = {word:i for i, word in enumerate(id2bigram_dvach)}\n",
    "\n",
    "\n",
    "for ngram in trigrams_dvach:\n",
    "    word1, word2, word3 = ngram.split()\n",
    "    bigram = word1 + ' ' + word2\n",
    "    matrix_dvach[bigram2id_dvach[bigram], word2id_dvach[word3]] = (trigrams_dvach[ngram]/bigrams_dvach[bigram])\n",
    "\n",
    "matrix_dvach = csr_matrix(matrix_dvach)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "e0bb4588",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_news = lil_matrix((len(bigrams_news), len(unigrams_news)))\n",
    "\n",
    "id2word_news = list(unigrams_news)\n",
    "word2id_news = {word:i for i, word in enumerate(id2word_news)}\n",
    "\n",
    "id2bigram_news = list(bigrams_news)\n",
    "bigram2id_news = {word:i for i, word in enumerate(id2bigram_news)}\n",
    "\n",
    "\n",
    "for ngram in trigrams_news:\n",
    "    word1, word2, word3 = ngram.split()\n",
    "    bigram = word1 + ' ' + word2\n",
    "    matrix_news[bigram2id_news[bigram], word2id_news[word3]] = (trigrams_news[ngram]/bigrams_news[bigram])\n",
    "\n",
    "matrix_news = csr_matrix(matrix_news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "7271d112",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(matrix, id2word, bigram2id, id2bigram, n=100, start='<start> <start>'):\n",
    "    text = []\n",
    "    current_idx = bigram2id[start]\n",
    "    \n",
    "    for i in range(n):\n",
    "        \n",
    "        chosen = np.random.choice(list(range(matrix.shape[1])), p=matrix[current_idx].toarray()[0])\n",
    "        text.append(id2word[chosen])\n",
    "        \n",
    "        if id2word[chosen] == '<end>':\n",
    "            chosen = bigram2id['<start> <start>']\n",
    "            \n",
    "        else:\n",
    "            bigram = id2bigram[current_idx]\n",
    "            trigram = bigram + ' ' + id2word[chosen]\n",
    "            w1, w2, w3 = trigram.split()\n",
    "            new_bigram = w2 + ' ' + w3\n",
    "            chosen = bigram2id[new_bigram]\n",
    "            \n",
    "        current_idx = chosen\n",
    "    \n",
    "    return ' '.join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "90c2c609",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perplexity(probas):\n",
    "    p = np.exp(np.sum(probas))\n",
    "    N = len(probas)\n",
    "    \n",
    "    return p**(-1/N) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "a97ae009",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_perplexity(phrase):\n",
    "    \n",
    "    prob = {'dvach':[], 'news':[]}\n",
    "\n",
    "    for ngram in ngrammer(['<start>'] + ['<start>'] + normalize(phrase) + ['<end>']):\n",
    "        word1, word2, word3 = ngram.split()\n",
    "        bigram = word1 + ' ' + word2\n",
    "        if bigram in bigrams_dvach and ngram in trigrams_dvach:\n",
    "            prob['dvach'].append(np.log(trigrams_dvach[ngram]/bigrams_dvach[bigram]))\n",
    "        else:\n",
    "            prob['dvach'].append(np.log(0.00001))\n",
    "    \n",
    "        if bigram in bigrams_news and ngram in trigrams_news:\n",
    "            prob['news'].append(np.log(trigrams_news[ngram]/bigrams_news[bigram]))\n",
    "        else:\n",
    "            prob['news'].append(np.log(0.00001))\n",
    "            \n",
    "    return perplexity(prob['dvach']), perplexity(prob['news'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76b83c3",
   "metadata": {},
   "source": [
    "### Примеры сгенерированных текстов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "defcacf1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "с черепно-мозговой травмой полонский в тяжелом состоянии госпитализирована беременная женщина \n",
      " об этом сегодня на всероссийском совещании по проблемам науки заявил в среду постоянный представитель израиля при оон иегуда ланкри сказал что эта программа \n",
      " по оценке журнала интернет и другие комплектующие увеличились в рублевом исчислении в 2,504 раза более 540 мирных жителей в тыл врага передает нтв \n",
      " как сообщает wired представители havenco уже заявили о том что по предварительным сведениям сход вагонов произошел не на что уйдет еще несколько недель выделение второго транша кредита заявил заместитель главы мвд михаил ваничкин сообщает риа новости таможенные органы россии откажутся от\n"
     ]
    }
   ],
   "source": [
    "print(generate(matrix_news, id2word_news, bigram2id_news, id2bigram_news).replace('<end>', '\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "bedee802",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "что не все целиком \n",
      " там даже скипать нельзя \n",
      " потому что не так как последние будут продавать свои васянские ретекстуры на беспезда \n",
      " не важно на что вы будете делать если слишком жарко — выходите из лавки на узкую ца \n",
      " тут был один случай или это говно собираются сносить \n",
      " по моему опыту да там одни школьнички травят других школьничков как-то по другому что то ещё терпимо \n",
      " и игра на ведроиде \n",
      " тебя по штанине течёт чмоша-хейтер без аргументов \n",
      " значит скачит на мистеческом хую шамана гадальщика \n",
      " какой подход выбирать \n",
      " для меня тян пустое место\n"
     ]
    }
   ],
   "source": [
    "print(generate(matrix_dvach, id2word_dvach, bigram2id_dvach, id2bigram_dvach).replace('<end>', '\\n'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4da783",
   "metadata": {},
   "source": [
    "### Рассчет перплексии на тестовой выборке из 10 текстов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "f0a292a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Значения перплексии текстов:\n",
      "346.04299406789534\n",
      "1966.613308078042\n",
      "23703.411851246256\n",
      "23671.503714242263\n",
      "61822.15191056624\n",
      "44788.894907421505\n",
      "1193.706497445341\n",
      "19.399890839584437\n",
      "118.83316294206847\n",
      "108.6707949062014\n"
     ]
    }
   ],
   "source": [
    "print('Значения перплексии текстов:')\n",
    "for text in test_dvach:\n",
    "    print(text_perplexity(text)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "595d1b43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Значения перплексии текстов:\n",
      "9075.506695032758\n",
      "2221.207237303329\n",
      "21616.280010688606\n",
      "4756.515099496046\n",
      "321.9359397055297\n",
      "18938.12599022405\n",
      "3794.061544807548\n",
      "50443.03421515644\n",
      "18132.144650748425\n",
      "23948.160623023487\n"
     ]
    }
   ],
   "source": [
    "print('Значения перплексии текстов:')\n",
    "for text in test_news:\n",
    "    print(text_perplexity(text)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0a8dd5",
   "metadata": {},
   "source": [
    "## Задание № 2* (2 балла). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b36c44b",
   "metadata": {},
   "source": [
    "Прочитайте главу про языковое моделирование в книге Журафски и Мартина - https://web.stanford.edu/~jurafsky/slp3/3.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9b1bd8",
   "metadata": {},
   "source": [
    "Развернуто (в пределах 1000 знаков) ответьте на вопросы (по-русски):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c2cf844",
   "metadata": {},
   "source": [
    "1. Что можно делать с проблемой несловарных слов? В семинаре мы просто использовали какое-то маленькое значение вероятности, а какие есть другие способы?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "046edd8d",
   "metadata": {},
   "source": [
    "Вместо того, чтобы приписать слову маленькое значение вероятности, можно добавить в словарь фиктивное слово \\<UNK\\>. Есть 2 способа определить вероятность такого слова:\n",
    "* если словарь уже построен, все несловарные слова заменяем на \\<UNK\\> на этапе нормализации и считаем вероятность \\<UNK\\> как в случае с любым другим ловом\n",
    "* если словаря нет, создаем словарь, в который включаем слова, имеющие вероятности выше выбранного порога, либо входящие в топ по вероятности (например, можно брать первые 10000 слов); добавляем \\<UNK\\> в словарь; слова, не вошедшие в словарь, заменяются на \\<UNK\\>, вероятность которого считается так же, как в случае 1). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d1c152",
   "metadata": {},
   "source": [
    "2. Что такое сглаживание (smoothing)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb424f4",
   "metadata": {},
   "source": [
    "Бывают случаи, когда слово присутствует в словаре, но в тестовом тексте появляется в неизвестном контексте (например, слово \"змей\" есть в словаре, но оно никогда не появлялось после слова \"тугарин\". Если допустить, что в словаре есть все варианты контекстов, то языковая модель присвоит такому случаю нулевую вероятность. Чтобы этого избежать, необходимо оставить долю вероятности для таких событий, \"отщепнув\" ее от вероятности наиболее частых контекстов. Такое перераспределение вероятностей называется **сглаживанием**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
