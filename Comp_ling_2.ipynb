{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3705663",
   "metadata": {},
   "source": [
    "# Домашнее задание № 2. Мешок слов"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf72d19",
   "metadata": {},
   "source": [
    "## Задание 1 (3 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a045e99",
   "metadata": {},
   "source": [
    "У векторайзеров в sklearn есть встроенная токенизация на регулярных выражениях. Найдите способ заменить её на кастомную токенизацию"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b4d453",
   "metadata": {},
   "source": [
    "Обучите векторайзер с дефолтной токенизацией и с токенизацией razdel.tokenize. Обучите классификатор с каждым из векторизаторов. Сравните метрики и выберете победителя. \n",
    "\n",
    "(в вашей тетрадке должен быть код обучения и все метрики; если вы сдаете в .py файлах то сохраните полученные метрики в отдельном файле или в комментариях)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "129c4d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from nltk.corpus import stopwords\n",
    "from razdel import tokenize\n",
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8bbc2365",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('labeled.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd3adfa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(data, test_size=0.1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63315440",
   "metadata": {},
   "outputs": [],
   "source": [
    "def razdel_tokenizer(text):\n",
    "    lst = list(tokenize(text))\n",
    "    return [word.text for word in lst]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1009,
   "id": "3df99cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_vect = TfidfVectorizer(min_df=10, max_df=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1010,
   "id": "63b42a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_default = default_vect.fit_transform(train.comment)\n",
    "X_default_test = default_vect.transform(test.comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1011,
   "id": "43f839da",
   "metadata": {},
   "outputs": [],
   "source": [
    "razdel_vect = TfidfVectorizer(min_df=10, max_df=0.4, tokenizer = razdel_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1012,
   "id": "359d1fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_razdel = razdel_vect.fit_transform(train.comment)\n",
    "X_razdel_test = razdel_vect.transform(test.comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1013,
   "id": "e35eacee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((12970, 3448), (1442, 3448))"
      ]
     },
     "execution_count": 1013,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_razdel.shape, X_razdel_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1014,
   "id": "b8681ac4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((12970, 3383), (1442, 3383))"
      ]
     },
     "execution_count": 1014,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_default.shape, X_default_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1015,
   "id": "0cf27d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train.toxic.values\n",
    "y_test = test.toxic.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1017,
   "id": "317058bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = DecisionTreeClassifier(max_depth=8, class_weight='balanced')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1018,
   "id": "058501e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.82      0.50      0.62       982\n",
      "         1.0       0.42      0.76      0.54       460\n",
      "\n",
      "    accuracy                           0.58      1442\n",
      "   macro avg       0.62      0.63      0.58      1442\n",
      "weighted avg       0.69      0.58      0.59      1442\n",
      "\n"
     ]
    }
   ],
   "source": [
    "classifier.fit(X_default, y)\n",
    "preds_default = classifier.predict(X_default_test)\n",
    "print(classification_report(y_test, preds_default))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1019,
   "id": "923b0d19",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.84      0.41      0.55       982\n",
      "         1.0       0.40      0.83      0.54       460\n",
      "\n",
      "    accuracy                           0.54      1442\n",
      "   macro avg       0.62      0.62      0.54      1442\n",
      "weighted avg       0.70      0.54      0.54      1442\n",
      "\n"
     ]
    }
   ],
   "source": [
    "classifier.fit(X_razdel, y)\n",
    "preds_razdel = classifier.predict(X_razdel_test)\n",
    "print(classification_report(y_test, preds_razdel))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf3969d",
   "metadata": {},
   "source": [
    "Вывод: классификатор с токенизацией razdel в целом чуть лучше с точки зрения f1 и recall, чем классификатор со встроенной токенизацией, precision почти не отличается. Хотя прогоны каждый раз дают разные результаты, так что можно сказать, что разницы вовсе нет или она очень несущественна."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ffa9f76",
   "metadata": {},
   "source": [
    "## Задание 2 (3 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f358949",
   "metadata": {},
   "source": [
    "Преобразуйте таблицу с абсолютными частотностями в семинарской тетрадке в таблицу с tfidf значениями. (Таблица - https://i.ibb.co/r5Nc2HC/abs-bow.jpg) Формула tfidf есть в семинаре на картнике с пояснениями на английском. \n",
    "Считать нужно в питоне. Формат итоговой таблицы может быть любым, главное, чтобы был код и можно было воспроизвести вычисления. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1020,
   "id": "c5b50abc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>я</th>\n",
       "      <th>ты</th>\n",
       "      <th>и</th>\n",
       "      <th>только</th>\n",
       "      <th>не</th>\n",
       "      <th>он</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>я и ты</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ты и я</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>я, я и только я</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>только не я</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>он</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 я  ты  и  только  не  он\n",
       "я и ты           1   1  1       0   0   0\n",
       "ты и я           1   1  1       0   0   0\n",
       "я, я и только я  3   0  1       1   0   0\n",
       "только не я      1   0  0       1   1   0\n",
       "он               0   0  0       0   0   1"
      ]
     },
     "execution_count": 1020,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame({\"я\" : [1, 1, 3, 1, 0],\n",
    "                 \"ты\" : [1, 1, 0, 0, 0],\n",
    "                 \"и\" : [1, 1, 1, 0, 0],\n",
    "                 \"только\" : [0, 0, 1, 1, 0],\n",
    "                 \"не\" : [0, 0, 0, 1, 0],\n",
    "                 \"он\" : [0, 0, 0, 0, 1]})\n",
    "df = df.rename({0 : \"я и ты\", 1 : \"ты и я\", 2 : \"я, я и только я\", 3 : \"только не я\", 4 : \"он\"})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1021,
   "id": "a79d7df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_tfidf(df):\n",
    "    values = []\n",
    "    for row in df.index:\n",
    "        doc_tfidf = []\n",
    "        for column in df.columns:\n",
    "            # берем значение в ячейке:\n",
    "            doc_tfidf.append(df[column].loc[row] / \\\n",
    "            # делим на количество слов в тексте (сумма по строке) - получаем tf\n",
    "            df.loc[row].sum() * \\\n",
    "            # tf умножаем на idf: логарифм от частного - количество текстов в коллекции (первая размерность матрицы)\n",
    "            math.log(df.shape[0] / \\\n",
    "                     # делим на количество текстов в коллекции с данным токеном (количество ненулевых значений в столбце)\n",
    "                     np.count_nonzero(df[column])))\n",
    "        values.append(doc_tfidf)\n",
    "    return pd.DataFrame(values, index = df.index, columns = df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1022,
   "id": "f7185485",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>я</th>\n",
       "      <th>ты</th>\n",
       "      <th>и</th>\n",
       "      <th>только</th>\n",
       "      <th>не</th>\n",
       "      <th>он</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>я и ты</th>\n",
       "      <td>0.074381</td>\n",
       "      <td>0.30543</td>\n",
       "      <td>0.170275</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ты и я</th>\n",
       "      <td>0.074381</td>\n",
       "      <td>0.30543</td>\n",
       "      <td>0.170275</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>я, я и только я</th>\n",
       "      <td>0.133886</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.102165</td>\n",
       "      <td>0.183258</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>только не я</th>\n",
       "      <td>0.074381</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.305430</td>\n",
       "      <td>0.536479</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>он</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.609438</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        я       ты         и    только        не        он\n",
       "я и ты           0.074381  0.30543  0.170275  0.000000  0.000000  0.000000\n",
       "ты и я           0.074381  0.30543  0.170275  0.000000  0.000000  0.000000\n",
       "я, я и только я  0.133886  0.00000  0.102165  0.183258  0.000000  0.000000\n",
       "только не я      0.074381  0.00000  0.000000  0.305430  0.536479  0.000000\n",
       "он               0.000000  0.00000  0.000000  0.000000  0.000000  1.609438"
      ]
     },
     "execution_count": 1022,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_tfidf(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5bc8de",
   "metadata": {},
   "source": [
    "## Задание 3 (2 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8961bbf",
   "metadata": {},
   "source": [
    "Обучите 2 любых разных классификатора из семинара. Предскажите токсичность для текстов из тестовой выборки (используйте одну и ту же выборку для обоих классификаторов) и найдите 10 самых токсичных для каждого из классификаторов. Сравните получаемые тексты - какие тексты совпадают, какие отличаются, правда ли тексты токсичные?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46681ef",
   "metadata": {},
   "source": [
    "Требования к классификаторам:   \n",
    "а) один должен использовать CountVectorizer, другой TfidfVectorizer  \n",
    "б) у векторазера должны быть вручную заданы как минимум 5 параметров  \n",
    "в) у классификатора должно быть задано вручную как минимум 2 параметра  \n",
    "г)  f1 мера каждого из классификаторов должна быть минимум 0.75  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ed77d7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = CountVectorizer(max_df=0.35, max_features=3000, binary=True, tokenizer=razdel_tokenizer, ngram_range=(1,2))\n",
    "tfidf = TfidfVectorizer(min_df=10, max_df=0.4, sublinear_tf=True, smooth_idf=False, ngram_range=(1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "87af92e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_2, test_2 = train_test_split(data, test_size=0.1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fde4b6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_count_train = count.fit_transform(train_2.comment)\n",
    "X_count_test = count.transform(test_2.comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0f4c3b45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((12970, 3000), (1442, 3000))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_count_train.shape, X_count_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "225820dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tfidf_train = tfidf.fit_transform(train_2.comment)\n",
    "X_tfidf_test = tfidf.transform(test_2.comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8cd9e19b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((12970, 4645), (1442, 4645))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tfidf_train.shape, X_tfidf_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1d7152ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train_2.toxic.values\n",
    "y_test = test_2.toxic.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b4699192",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(C=0.2, class_weight='balanced')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1119,
   "id": "7cd916f1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.90      0.78      0.83       930\n",
      "         1.0       0.68      0.84      0.75       512\n",
      "\n",
      "    accuracy                           0.80      1442\n",
      "   macro avg       0.79      0.81      0.79      1442\n",
      "weighted avg       0.82      0.80      0.80      1442\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr.fit(X_count_train, y_train)\n",
    "preds_count = lr.predict(X_count_test)\n",
    "print(classification_report(y_test, preds_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1120,
   "id": "276ebbca",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = MultinomialNB(alpha=0.7, fit_prior = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1121,
   "id": "0aadc3f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.88      0.88      0.88       930\n",
      "         1.0       0.78      0.78      0.78       512\n",
      "\n",
      "    accuracy                           0.84      1442\n",
      "   macro avg       0.83      0.83      0.83      1442\n",
      "weighted avg       0.84      0.84      0.84      1442\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nb.fit(X_tfidf_train, y_train)\n",
    "preds_tfidf = nb.predict(X_tfidf_test)\n",
    "print(classification_report(y_test, preds_tfidf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1122,
   "id": "3708746b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Какие же хохлы всадники, пиздец просто.\n",
      " Вероятность: 0.9978602747497146 | Метка: 1.0\n",
      "\n",
      "По мексикански Флаг: Ублюдок, мать твою, а ну иди сюда говно собачье, решил меня поднять? Ты, засранец вонючий, мать твою, а? Ну иди сюда, попробуй меня поднять, я тебя сам подниму ублюдок, онанист чертов, будь ты проклят, иди идиот, трахать тебя и всю семью, говно собачье, жлоб вонючий, дерьмо, сука, падла, иди сюда, мерзавец, негодяй, гад, иди сюда ты - говно, ЖОПА!\n",
      " Вероятность: 0.9970673815700962 | Метка: 1.0\n",
      "\n",
      "Какие же хохлы новозеландцы дегенераты.\n",
      " Вероятность: 0.99644339264494 | Метка: 1.0\n",
      "\n",
      "null 0 Сука, какие же коммибляди тупые.\n",
      " Вероятность: 0.9951049763907913 | Метка: 1.0\n",
      "\n",
      "Нахуй иди, я тебе весь тред что ли читать буду? Пидор, бешбармак тебе в хычин!\n",
      " Вероятность: 0.9942895248242882 | Метка: 1.0\n",
      "\n",
      "ВОЖДЬ Я УБЬЮ ТЕБЯ СУКА\n",
      " Вероятность: 0.9902176083514301 | Метка: 1.0\n",
      "\n",
      "Мерзкий, тупой но сука смешной\n",
      " Вероятность: 0.9887629671893426 | Метка: 1.0\n",
      "\n",
      "АХАХАХАХАХАХАХ Ты в очередной раз доказываешь насколько ты тупой долбоёб.\n",
      " Вероятность: 0.9885775190370606 | Метка: 1.0\n",
      "\n",
      "Шлюха. Это парад шлюх.\n",
      " Вероятность: 0.9879629118636928 | Метка: 1.0\n",
      "\n",
      "Эреб, пидор, а не гей.\n",
      " Вероятность: 0.984950045551618 | Метка: 1.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "probas_toxic = pd.DataFrame()\n",
    "probas_toxic['comment'] = test_2.comment\n",
    "probas_toxic['toxic_probas'] = nb.predict_proba(X_tfidf_test)[:,1]\n",
    "for i in range(10):\n",
    "    print(probas_toxic.sort_values('toxic_probas', ascending = False).iloc[i][0], 'Вероятность:', \n",
    "         probas_toxic.sort_values('toxic_probas', ascending = False).iloc[i][1], \"| Метка:\",\n",
    "          test_2.toxic.values[test_2.comment == probas_toxic.sort_values('toxic_probas', ascending = False).iloc[i][0]][0], \n",
    "          end = '\\n\\n')\n",
    "# все комментарии действительно токсичны"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1124,
   "id": "2bcba868",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "та ну, хуйня это все про джентельменство . меня в свое время эти брачные игрища так сильно заебали, что я вообще перестал париться и начал себя вести так, как и обычно. без всех этих ужимок и образа славного парня . без зазрения совести перед девушками курил, харкался, матерился как сапожник, рассказывал охуитительные истории о том, как когда-то обосрался бухал ебал кого-то проснулся с обблеванной тян пересказывал пасты про говно и т.д. т.е вел себя как быдлан, как обычно я себя и веду, когда вокруг никого нет. и тогда девушки начали тянуться. после этого я сделал для себя вывод, что девушки вообще не воспринимают что ты им говоришь, что делаешь и т.д. они воспринимают лишь то, как ты все это говоришь и делаешь. когда я начал вести себя как обычно, я делал это, блять, уверенно, самовлюбленно, нагло и аки самец . может это будило в девушках какие-то там инстинкты и они думали что-то в стиле ну ничего что быдло, зато какой уверенный, ВАХ просто! или я хуй знает, но такое поведение позволило мне пользоваться определенной популярностью у девушек. вообще если покопаться во всей этой теме, образ отношений, любви, поведения во всех этих брачных игрищах, все это навязывается нам с рождения источниками массовой информации, через книги, фильмы, красивые любовные истории и т.д. веди себя как джентельмен, жди ту, которая встрепетнет твою душу и от которой твое сердце забьется чаще, а в животе будут летать бабочки, положи всю свою жизнь на счастье второй половинки, будь для нее горой, добытчиком, ебырем-террористом и вся эта прочая романтичная, сопливая хуета. и все это в корне не верно. реальная любовь и отношения, не имеют ничего общего с тем, что нам с рождения вбивают в голову. все эти высокие чувства - обычное желание ебаться. видишь человека - получаешь дозу гормона - кайфуешь. не видишь человека - нет дозы гормона - появляется ломка - ищешь способы увидеть человека. вот тебе описание любви. через какое-то время эти чувства угаснут так или иначе и если у вас кроме этих чуйств нихуя не было, то развод и девичья фамилия. настоящая любовь, это когда у вас есть тысячи причин быть вместе, помимо того, что у вас стойкое желание ебаться. даже когда желание ебаться уже прошло, вам все равно приятно находиться вместе. такое же навязывание светлой и романтишной любви , на моей памяти, вызывало не мало проблем у моих знакомых девушек. они втупую сидели и ждали принца на белом коне ГОДАМИ, ссылаясь на охуенные фразочки если твое, то ты не пропустишь , обещанного три года ждут и прочую ебанину. я и сам к некоторым пытался лезть как кандидат в парни, ухаживал, звал гулять, пытался побольше времени проводить с ними, но все упиралось в одну хуйню. а именно вот есть парень, он очень популярен, он вообще не мой уровень, у него целые армии из девушек желающих попрыгать на его куканчике, и вот она я вся такая красивая, посижу на попе смирно, нихуя не буду делать, он обязательно меня заметит, а пока что я буду отметать кучу нормальных вариантов отношений! ох бедная я несчастная, парня найти то себе не могу, 25 лет а все в девках хожу! . а все почему? а потому что им с детства вбивали, что ухаживать должны мальчики, подкатывать должны мальчики, а ты, вся такая красивая, должна сидеть и ждать пока к тебе подкатят и лишь отсеивать нормальных парней в ожидании его - ПРЫНЦА НА БЕЛОМ КОНЕ!!1 именно так любовь и работает, нахуй!!11! чето меня занесло и печет пиздец. все это джентельменство хуйня собачья. образ современной любви - та еще дичь. надеюсь через пару десятков поколений, весь этот романтизм и брачные игрища станут архаизмом и люди просто будут влюбляться и быть вместе, без всего этого социально-любовного шлака.\n",
      " Вероятность: 0.9999997361172136 | метка: 0.0\n",
      "\n",
      "Поясняю за Била. Те, кто его смотрит отморозки, или быдло. Они просто не могут представить, что могут оказаться в такой ситуации. А для них омежки это скот, но когда затрагивают их ЧСВ совершенно безобидным пранком, когда облили водой раздвиженцев, то быдло и отморозки встают в позу и говорят о НАРУШЕНИЕ ГРАНИЦ. Тем, кому припекает омежки, которые из-за низкой самооценки неосознанно представляют себя на месте героев пранка. А потом в своих влажных фантазия избивают этого пранкера или пытают его. А нормальные люди просто это не смотрят. А если и посмотрят то просто скажут, что он дурак. Бил же трус и быдло -- доебывается обычно до слабых, что свойственно гопоте. Очень удивилась, что у него так много лайков на видосах. Ну видимо это те, кто любят Хованского, Мопса и прочих быдлоютуберов. Не удивлюсь, что Била смотрят в основном не из приличных городов, а из всяких мухасрансков. Вывод, русские терпилы будут терпеть до конца. Как лев против или стопхам(который уже окуклился). Включу либераста, как видите это типичный руSSкий мир, мы готовы превозносить любое быдло в герои, а потом оправдывать его. С Билом ниче не случится, я гарантирую это. Он явно не тупой и точно знает, где граница дозволенного, то есть мы не увидим его с ЛГБТ-флагом в Чечне, например. Так что, дорогие руSSкие сосите и дальше хуй и терпите. Мимо проходила\n",
      " Вероятность: 0.9999470008657007 | метка: 1.0\n",
      "\n",
      "По мексикански Флаг: Ублюдок, мать твою, а ну иди сюда говно собачье, решил меня поднять? Ты, засранец вонючий, мать твою, а? Ну иди сюда, попробуй меня поднять, я тебя сам подниму ублюдок, онанист чертов, будь ты проклят, иди идиот, трахать тебя и всю семью, говно собачье, жлоб вонючий, дерьмо, сука, падла, иди сюда, мерзавец, негодяй, гад, иди сюда ты - говно, ЖОПА!\n",
      " Вероятность: 0.9999314612877367 | метка: 1.0\n",
      "\n",
      "Стас, никому, кроме тебя и армии твоих подсосов(которые представляют собой типичный дегенеративный биомусор, ведущийся на любые скандалы-интриги), твои ролики нахуй не нужны. Серьёзно, ты сделал новости с целью показать, что такое говно может делать любой, а аудитория осталась на том же уровне, ведь людям извне ты не интересен. Да ещё и просит не подписываться, чтобы такую-то годноту ложкой хлебать подольше. Ты обосрался, стал посмешищем для абсолютно всех ютуберов, которые не являются полными ебланами. Тот же Хованский не ссыт тебе на ебало только потому, что ты вертишься с ним в одной компании, иногда даже лично пересекаетесь. Приятно было слышать, как он говорил, что отстреливался бы от таких, как ты, из огнестрела, стараясь забрать с собой побольше коммунистов, когда они придут его оаскулачиапть? Он открыто хуесосил людей и за меньшие грехи. Сложи 2 и 2, как он к тебе относится на самом деле. После чего ты сделал ещё более смешной ролик, где истеришь как побитая шлюха во время ПМС. Я ПОДЕБИЛ, А ЕСЛИ ВЫ НЕ ПОНЯЛИ ЭТОГО, ТО ВЫ ТУПЫЕ . Ты мог хотя бы сам его посмотреть перед заливом? Мне даже рофлить над тобой расхотелось, из смешного дегенерата, ты стал жалким дурачком. Это как смеятся над роликами, где контуженные ветераны пытаются ходить под клубную музыку. Над неполноценными смеятся плохо, даже стыдно стало. Я не утрирую. Просто посмотри на себя, Стас. Ну правда. Банишь людей в группе за лвйки и одно упоминание стрима. Ты делаешь всё, в чём самый отбитый и дегенеративный либераст обвиняет совок и сверкаешь разорванным очком. Никто тебя несправедливо не обсирал. Что на стриме по поводу дат, ну ты же сам проебался. На подкасте сообщил, что не будешь стримить. Если ты не был уверен, то зачем это говорить? А если был, почему не сообщил Маргиналу сразу же? Твоё слово в целом не стоит нихуя. Обещаешь не банить-куча удалённых комментов. Обещаешь стрим-не идёшь. Обещаешь что-то ещё, всегда проёбываешься, всё чаще на нарушение обещания тебе нужно в районе секунды-дня. Стоит ли удивлятся, что тебе за это прилетело? Когда то должно было. Ты сам срёшь себе в штаны, не злись, когда на это указывают пальцем. Вероятность: 0.9999192792044446 | метка: 1.0\n",
      "\n",
      "Я тебе дохуя пруфов привёл, а ты ебанный дебил не верил. Сося должен был вам сказать, но сося долбаёб и тебя за человека не держит, почему он тебе должен рассказывать о своих фейл, если его заявляения раньше звучали гордо . Ебать ты дибил.\n",
      " Вероятность: 0.999813167936672 | метка: 1.0\n",
      "\n",
      "Нахуй иди, я тебе весь тред что ли читать буду? Пидор, бешбармак тебе в хычин!\n",
      " Вероятность: 0.998834945342395 | метка: 1.0\n",
      "\n",
      "Сука, ты ему про аномалии, а он тебе ПРО ХАБАР! ПРИЧЕМ ТУТ РЕЗАНЬЕ ПОЛОВЫХ ОРГАНОВ И ИЗНАСИЛОВАНИЯ, А ДЕБИЧ? ПРИРОДА ДАЛА МУЖЧИНАМ СИЛУ И МОЗГИ, ЧЕГО ЖЕНЩИНЫ СМОГУТ ПРИОБРЕСТИ ТОЛЬКО ЧЕРЕЗ ГИГАНТСКОЕ УПОРСТВО . Здесь речь не о дискриминации, А О БАНАЛЬНОМ ФАКТЕ ! Ну а кукареканье о равенстве прав - всего лишь КУКАРЕКАНЬЯ , я только представлю себе бабу металлурга и уже смешно становится, нет у них способностей к мужским профессиями - НУ И НЕХУЙ ТУДА ПРОСИТЬСЯ . Пускай есть готовят, за больными ухаживают, одежду шьют, картины рисуют и всё остальное, чем они обычно занимаются, НО РАВЕНСТВА В ПРАВАХ, ПОМИМО РАВЕНСТВА ЮРИДИЧЕСКИХ - ЭТО ПРОСТО АБСУРД. Если эта Фелмке Халсема топит за равенство, ТАК ПУСКАЙ БЕРЕТ АВТОМАТ В РУКИ И ПИЗДУЕТ ЗАЩИЩАТЬ СВОЮ СТРАНУ. ДЕЛАЕТ БЛЯДЬ МАРШ-БРОСКИ КАЖДЫЙ ДЕНЬ, РОЕТ ОКОПЫ, ПИЗДИТСЯ, УЧИТСЯ СТРЕЛЯТЬ, КОЛОТЬ ЧЕЛОВЕКА В РУКОПАШНУЮ И БРОСАТЬСЯ НА АМБРАЗУРЫ, ДОКАЗЫВАЯ ЧТО ОНА НИЧЕМ НЕ ХУЖЕ. Но нет, она же способна только пиздеть и ущемлять мужиков в их законных правах, нарушая стойкую систему и порядок.\n",
      " Вероятность: 0.9985138088947393 | метка: 1.0\n",
      "\n",
      "Какая же ты мразота, надеюсь, ты как можно быстрее сдохнешь от спидорака. Такие в принципе не должны доживать до 25. Излишне этичным он стал, не ссать людям в напитки, не насиловать женщин, никого не бить - это теперь излишняя этичность. Блядь, каких же лучей поноса я тебе шлю, уебок.\n",
      " Вероятность: 0.9981096613586555 | метка: 1.0\n",
      "\n",
      "КОГДА Я ВИДУ ВАТНИКА, Я ЕГО ПОСЫЛАЮ! ТЫ ВАТНИК, ИДИ НА ХУЙ! ОБОССАЛ ВАШ ПЕТУШИНЫЙ ЗАГОН!\n",
      " Вероятность: 0.9980187601961293 | метка: 1.0\n",
      "\n",
      "Страницу обнови, дебил. Это тоже не оскорбление, а доказанный факт - не-дебил про себя во множественном числе писать не будет. Или мы в тебя верим - это ты и твои воображаемые друзья?\n",
      " Вероятность: 0.997550154372875 | метка: 1.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "probas_toxic_1 = pd.DataFrame()\n",
    "probas_toxic_1['comment'] = test_2.comment\n",
    "probas_toxic_1['toxic_probas'] = lr.predict_proba(X_count_test)[:,1]\n",
    "for i in range(10):\n",
    "    print(probas_toxic_1.sort_values('toxic_probas', ascending = False).iloc[i][0], 'Вероятность:', \n",
    "         probas_toxic_1.sort_values('toxic_probas', ascending = False).iloc[i][1], '| метка:',\n",
    "          test_2.toxic.values[test_2.comment == probas_toxic_1.sort_values('toxic_probas', ascending = False).iloc[i][0]][0], \n",
    "          end = '\\n\\n')\n",
    "# один большой комментарий (первый) на самом деле не является токсичным (по метке) \n",
    "# возможно, он был отнесен в эту категорию из-за большого количества мата\n",
    "# остальные действительно токсичны\n",
    "# только одно совпадение: \"Нахуй иди, я тебе весь тред что ли читать буду? Пидор, бешбармак тебе в хычин!\"\n",
    "# почему-то в топе у байеса в основном короткие комментарии, у лог. регрессии - длинные и средней длины"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68324753",
   "metadata": {},
   "source": [
    "## *Задание 4 (2 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7794f97",
   "metadata": {},
   "source": [
    "Для классификаторов LogisticRegression, Decision Trees, Naive Bayes, Random Forest найдите способ извлечь важность признаков для предсказания токсичного класса. Сопоставьте полученные числа со словами (или нграммами) в словаре и найдите топ - 5 \"токсичных\" слов для каждого из классификаторов. \n",
    "\n",
    "Важное требование: в топе не должно быть стоп-слов. Для этого вам нужно будет правильным образом настроить векторизацию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1621065d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_3, test_3 = train_test_split(data, test_size=0.1, shuffle=True)\n",
    "stop = stopwords.words('russian')\n",
    "vect = CountVectorizer(max_df=0.35, max_features=3000, binary=True, tokenizer=razdel_tokenizer, stop_words=stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "32277425",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = vect.fit_transform(train_3.comment)\n",
    "test_X = vect.transform(test_3.comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2213808a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = train_3.toxic.values\n",
    "test_y = test_3.toxic.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "aca97a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "nbc = MultinomialNB(alpha=0.7)\n",
    "nbc.fit(train_X, train_y)\n",
    "preds_nb = nbc.predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "5ab0cc8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('даун', -11.718928670541132),\n",
       " ('ебаные', -11.718928670541132),\n",
       " ('пидарасы', -11.718928670541132),\n",
       " ('сука', -11.718928670541132),\n",
       " ('пидорашка', -11.718928670541132)]"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# создаем словарь, где ключ - индекс, значение - важность признака\n",
    "# по индексу сопоставляем каждому слову показатель его важности\n",
    "nbc_values = dict(enumerate(nbc.feature_log_prob_[0]))\n",
    "dict_nbc = {}\n",
    "for w in vect.vocabulary_:\n",
    "    dict_nbc[w] = nbc_values[vect.vocabulary_[w]]\n",
    "# отсортируем словарь по значению\n",
    "toxic_nbc = {}\n",
    "sorted_keys = sorted(dict_nbc, key=dict_nbc.get) \n",
    "for w in sorted_keys:\n",
    "    toxic_nbc[w] = dict_nbc[w]\n",
    "\n",
    "list(toxic_nbc.items())[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "eaf9062a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lreg = LogisticRegression(C=0.2, class_weight='balanced')\n",
    "lreg.fit(train_X, train_y)\n",
    "preds_lr = lreg.predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "74057de4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('хохлы', 1.890095370868237),\n",
       " ('хохлов', 1.8751993397986741),\n",
       " ('быдло', 1.4229512603162613),\n",
       " ('русских', 1.4127919975985022),\n",
       " ('тупые', 1.3900318761908554)]"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lreg_values = dict(enumerate(lreg.coef_[0]))\n",
    "dict_lreg = {}\n",
    "for w in vect.vocabulary_:\n",
    "    dict_lreg[w] = lreg_values[vect.vocabulary_[w]]\n",
    "\n",
    "toxic_lreg = {}\n",
    "sorted_keys = sorted(dict_lreg, key=dict_lreg.get, reverse=True) \n",
    "for w in sorted_keys:\n",
    "    toxic_lreg[w] = dict_lreg[w]\n",
    "\n",
    "list(toxic_lreg.items())[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "42171feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier()\n",
    "dt.fit(train_X, train_y)\n",
    "preds_dt = dt.predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "966c33b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(')', 0.027452048986981083),\n",
       " ('хохлы', 0.01437896049204086),\n",
       " ('тебе', 0.01301802053921191),\n",
       " ('нахуй', 0.012753267425720673),\n",
       " ('хохлов', 0.01152831771037857)]"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt_values = dict(enumerate(dt.feature_importances_))\n",
    "dict_dt = {}\n",
    "for w in vect.vocabulary_:\n",
    "    dict_dt[w] = dt_values[vect.vocabulary_[w]]\n",
    "\n",
    "toxic_dt = {}\n",
    "sorted_keys_dt = sorted(dict_dt, key=dict_dt.get, reverse=True) \n",
    "for w in sorted_keys_dt:\n",
    "    toxic_dt[w] = dict_dt[w]\n",
    "\n",
    "list(toxic_dt.items())[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "2b366f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier()\n",
    "rf.fit(train_X, train_y)\n",
    "preds_rf = rf.predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "2157c236",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(')', 0.012651777926934919),\n",
       " ('хохлы', 0.01106267011651788),\n",
       " ('хохлов', 0.009616974301070353),\n",
       " ('тебе', 0.008097603706890636),\n",
       " ('(', 0.00792177600095888)]"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_values = dict(enumerate(rf.feature_importances_))\n",
    "dict_rf = {}\n",
    "for w in vect.vocabulary_:\n",
    "    dict_rf[w] = rf_values[vect.vocabulary_[w]]\n",
    "\n",
    "toxic_rf = {}\n",
    "sorted_keys_rf = sorted(dict_rf, key=dict_rf.get, reverse=True) \n",
    "for w in sorted_keys_rf:\n",
    "    toxic_rf[w] = dict_rf[w]\n",
    "\n",
    "list(toxic_rf.items())[:5]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
